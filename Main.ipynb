{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.stem import WordNetLemmatizer #for noise removal \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re #Regex Library\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizerObject = WordNetLemmatizer()\n",
    "labels=[]\n",
    "tweets=[]\n",
    "def remove_noise(input_text):\n",
    "    noise_list = [\"a\", \"about\", \"after\", \"all\", \"also\", \"an\", \"another\", \"any\", \"and\", \"are\", \"as\", \"and\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"between\", \"but\", \"both\", \"by\", \"came\", \"can\", \"come\", \"could \", \"did\", \"do\", \"each\", \"even\", \"for\", \"from\", \"further\", \"furthermore\", \"get\", \"got\", \"has\", \"had\", \"he\", \"have\", \"her\", \"here\", \"him\", \"himself\", \"his\", \"how\", \"hi\", \"however\",\"i\", \"if\", \"in\", \"into\", \"is\", \"it\", \"its\", \"indeed\", \"just\", \"like\", \"made\", \"many\", \"me\", \"might\", \"more\", \"moreover\", \"most\", \"much\", \"must\", \"my never\", \"not\", \"now of\", \"on\", \"only\", \"other\", \"our\", \"out\", \"or\", \"over\", \"said\", \"same\", \"see\", \"should\", \"since\", \"she\", \"some\", \"still\", \"such\", \"take\", \"than\", \"that\", \"the\", \"their\", \"them\", \"then\", \"there\", \"these\", \"therefore\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"thus\", \"under\", \"up\", \"was\", \"way\", \"we\", \"well\", \"were\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"will\", \"with\", \"would\", \"your\", \"null\"]\n",
    "    words = input_text.split() # Split words by space\n",
    "    noise_free_words = [word for word in words if word.lower() not in noise_list] #Get a list of non-noise words\n",
    "    noise_free_text = \" \".join(noise_free_words) #Get a string of non-noise words\n",
    "    return noise_free_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_regex(input_text):\n",
    "    #split tweet by space\n",
    "    words = input_text.split() \n",
    "    regex_free_text=\"\"\n",
    "    #check if word is alpha(contain letters only) , then add it to regex_free_text\n",
    "    for word in words:\n",
    "      if word.isalpha():\n",
    "        #Lemmatization, on the other hand, is an organized & step by step procedure of obtaining\n",
    "        #the root form of the word, it makes use of vocabulary (dictionary importance of words) \n",
    "        #and morphological analysis (word structure and grammar relations).\n",
    "        #reduces the inflected words properly ensuring that the root word belongs to the language\n",
    "        #pos=\"V\"-->to give a root for each word !\n",
    "        regex_free_text += lemmatizerObject.lemmatize(word,pos=\"v\")\n",
    "        regex_free_text +=\" \"\n",
    "    return regex_free_text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(filename):\n",
    "    my_file = open(filename, mode='r')\n",
    "    #return value of csv file is an iterator \n",
    "    read = csv.reader(my_file,delimiter='\\t')\n",
    "    #splitting = read.split('\\t')\n",
    "    flag=0;\n",
    "    #flag ---> used to skip the header of the file\n",
    "    #column one for tweets , column 2 for our ouput(NOT or OFF)\n",
    "    for row in read:\n",
    "      if flag ==0:\n",
    "        flag=1\n",
    "        continue;\n",
    "      tweets.append(row[1])\n",
    "      labels.append(row[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(ourTweets,ourTestTweets):   \n",
    "   vectorizer = TfidfVectorizer()\n",
    "   # return value ---> position of the word , index of tweet , tfidf value of the word.\n",
    "   X = vectorizer.fit_transform(ourTweets)\n",
    "   Y = vectorizer.transform(ourTestTweets)\n",
    "   return X,Y;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVMClassifier(featuresTrain,labelsTrain,featuresTest):\n",
    "    #bn3ml training 3la train data (bn build our method 3aleha )\n",
    "    #f b3ml object mn classifier bt3i w b3den fit de bt3ml train lal data bt3ty\n",
    "    #tol---> nesbt el error el masbo7 beha el lw wsl 3ndha aw 2al yw2f w my7rksh el separator \n",
    "    #random_state is the seed used by the random number generator\n",
    "    #linear SVC da shbh precepton \n",
    "    clf = LinearSVC(random_state=0, tol=1e-5)\n",
    "    clf.fit(featuresTrain, labelsTrain)\n",
    "    #b predict b2a 3la test data bt3ty 3shn agib accuracy bt3t classifier da\n",
    "    X=clf.predict(featuresTest)\n",
    "    return X;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of svm calssifier:\n",
      "0.7253776435045317\n",
      "\n",
      "\n",
      "[[3804  618]\n",
      " [1200  998]]\n"
     ]
    }
   ],
   "source": [
    "def getAccuracy(outputLabels,actualLabels):\n",
    "      return accuracy_score(outputLabels, actualLabels)\n",
    "def calculateConfusionMatrix(outputLabels,actualLabels):\n",
    "    CM = confusion_matrix(actualLabels, outputLabels)\n",
    "    print(CM)\n",
    "readFile(\"offenseval-training-v1.tsv\")\n",
    "#loop for each tweet remove regex & noise\n",
    "for tweet in range(0,len(tweets)):\n",
    "    tweets[tweet]=remove_regex(tweets[tweet])\n",
    "    tweets[tweet]=remove_noise(tweets[tweet])\n",
    "#we have to divide our dataset into 2 parts (training data and test data)\n",
    "lenofLabel=(int)(len(labels)/2)\n",
    "lenofTweets=(int)(len(tweets)/2)\n",
    "train_labels=labels[:lenofLabel]\n",
    "test_labels=labels[lenofLabel:]\n",
    "train_tweets=tweets[:lenofLabel]\n",
    "test_tweets=tweets[lenofLabel:]\n",
    "#to extract Features\n",
    "features_train,features_test=extract_features(train_tweets,test_tweets)\n",
    "predictLabels=SVMClassifier(features_train,train_labels,features_test)\n",
    "#calculate accuracy of predicted labels (from our algorithm ) & the actual labels\n",
    "accuracy=getAccuracy(predictLabels,test_labels)\n",
    "print(\"accuracy of svm calssifier:\")\n",
    "print(accuracy)\n",
    "print(\"\\n\")\n",
    "      \n",
    "calculateConfusionMatrix(predictLabels,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
