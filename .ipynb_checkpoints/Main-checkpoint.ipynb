{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from nltk.stem import WordNetLemmatizer #for noise removal \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re #Regex Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizerObject = WordNetLemmatizer()\n",
    "labels=[]\n",
    "tweets=[]\n",
    "def remove_noise(input_text):\n",
    "    noise_list = [\"a\", \"about\", \"after\", \"all\", \"also\", \"an\", \"another\", \"any\", \"and\", \"are\", \"as\", \"and\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"between\", \"but\", \"both\", \"by\", \"came\", \"can\", \"come\", \"could \", \"did\", \"do\", \"each\", \"even\", \"for\", \"from\", \"further\", \"furthermore\", \"get\", \"got\", \"has\", \"had\", \"he\", \"have\", \"her\", \"here\", \"him\", \"himself\", \"his\", \"how\", \"hi\", \"however\",\"i\", \"if\", \"in\", \"into\", \"is\", \"it\", \"its\", \"indeed\", \"just\", \"like\", \"made\", \"many\", \"me\", \"might\", \"more\", \"moreover\", \"most\", \"much\", \"must\", \"my never\", \"not\", \"now of\", \"on\", \"only\", \"other\", \"our\", \"out\", \"or\", \"over\", \"said\", \"same\", \"see\", \"should\", \"since\", \"she\", \"some\", \"still\", \"such\", \"take\", \"than\", \"that\", \"the\", \"their\", \"them\", \"then\", \"there\", \"these\", \"therefore\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"thus\", \"under\", \"up\", \"was\", \"way\", \"we\", \"well\", \"were\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"will\", \"with\", \"would\", \"your\", \"null\"]\n",
    "    words = input_text.split() # Split words by space\n",
    "    noise_free_words = [word for word in words if word.lower() not in noise_list] #Get a list of non-noise words\n",
    "    noise_free_text = \" \".join(noise_free_words) #Get a string of non-noise words\n",
    "    return noise_free_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_regex(input_text):\n",
    "    #split tweet by space\n",
    "    words = input_text.split() \n",
    "    regex_free_text=\"\"\n",
    "    #check if word is alpha(contain letters only) , then add it to regex_free_text\n",
    "    for word in words:\n",
    "      if word.isalpha():\n",
    "        print(word+\"\\n\")\n",
    "        #Lemmatization, on the other hand, is an organized & step by step procedure of obtaining\n",
    "        #the root form of the word, it makes use of vocabulary (dictionary importance of words) \n",
    "        #and morphological analysis (word structure and grammar relations).\n",
    "        #example multiplying--->multiply\n",
    "        regex_free_text += lemmatizerObject.lemmatize(word)\n",
    "        print(lemmatizerObject.lemmatize(word)+ \"\\n\");\n",
    "        regex_free_text +=\" \"\n",
    "    return regex_free_text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(filename):\n",
    "    my_file = open(filename, mode='r')\n",
    "    #return value of csv file is an iterator \n",
    "    read = csv.reader(my_file,delimiter='\\t')\n",
    "    #splitting = read.split('\\t')\n",
    "    flag=0;\n",
    "    #flag ---> used to skip the header of the file\n",
    "    #column one for tweets , column 2 for our ouput(NOT or OFF)\n",
    "    for row in read:\n",
    "      if flag ==0:\n",
    "        flag=1\n",
    "        continue;\n",
    "      tweets.append(row[1])\n",
    "      labels.append(row[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features():   \n",
    "   vectorizer = TfidfVectorizer()\n",
    "   # return value ---> position of the word , index of tweet , tfidf value of the word.\n",
    "   X = vectorizer.fit_transform(tweets)\n",
    "   #print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go\n",
      "\n",
      "Go\n",
      "\n",
      "home\n",
      "\n",
      "home\n",
      "\n",
      "URL\n",
      "\n",
      "URL\n",
      "\n",
      "Amazon\n",
      "\n",
      "Amazon\n",
      "\n",
      "is\n",
      "\n",
      "is\n",
      "\n",
      "investigating\n",
      "\n",
      "investigating\n",
      "\n",
      "Chinese\n",
      "\n",
      "Chinese\n",
      "\n",
      "employees\n",
      "\n",
      "employee\n",
      "\n",
      "who\n",
      "\n",
      "who\n",
      "\n",
      "are\n",
      "\n",
      "are\n",
      "\n",
      "selling\n",
      "\n",
      "selling\n",
      "\n",
      "internal\n",
      "\n",
      "internal\n",
      "\n",
      "data\n",
      "\n",
      "data\n",
      "\n",
      "to\n",
      "\n",
      "to\n",
      "\n",
      "sellers\n",
      "\n",
      "seller\n",
      "\n",
      "looking\n",
      "\n",
      "looking\n",
      "\n",
      "for\n",
      "\n",
      "for\n",
      "\n",
      "an\n",
      "\n",
      "an\n",
      "\n",
      "edge\n",
      "\n",
      "edge\n",
      "\n",
      "in\n",
      "\n",
      "in\n",
      "\n",
      "the\n",
      "\n",
      "the\n",
      "\n",
      "competitive\n",
      "\n",
      "competitive\n",
      "\n",
      "URL\n",
      "\n",
      "URL\n",
      "\n",
      "Someone\n",
      "\n",
      "Someone\n",
      "\n",
      "this\n",
      "\n",
      "this\n",
      "\n",
      "piece\n",
      "\n",
      "piece\n",
      "\n",
      "of\n",
      "\n",
      "of\n",
      "\n",
      "shit\n",
      "\n",
      "shit\n",
      "\n",
      "to\n",
      "\n",
      "to\n",
      "\n",
      "a\n",
      "\n",
      "a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "readFile(\"offenseval-training-v1.tsv\")\n",
    "#loop for each tweet remove regex & noise\n",
    "for tweet in range(0,len(tweets)):\n",
    "    tweets[tweet]=remove_regex(tweets[tweet])\n",
    "    tweets[tweet]=remove_noise(tweets[tweet])\n",
    "#to extract Features\n",
    "features=extract_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
