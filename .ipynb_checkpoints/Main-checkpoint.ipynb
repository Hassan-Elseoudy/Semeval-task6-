{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re #Regex Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[]\n",
    "tweets=[]\n",
    "def remove_noise(input_text):\n",
    "    noise_list = [\"a\", \"about\", \"after\", \"all\", \"also\", \"an\", \"another\", \"any\", \"and\", \"are\", \"as\", \"and\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"between\", \"but\", \"both\", \"by\", \"came\", \"can\", \"come\", \"could \", \"did\", \"do\", \"each\", \"even\", \"for\", \"from\", \"further\", \"furthermore\", \"get\", \"got\", \"has\", \"had\", \"he\", \"have\", \"her\", \"here\", \"him\", \"himself\", \"his\", \"how\", \"hi\", \"however\",\"i\", \"if\", \"in\", \"into\", \"is\", \"it\", \"its\", \"indeed\", \"just\", \"like\", \"made\", \"many\", \"me\", \"might\", \"more\", \"moreover\", \"most\", \"much\", \"must\", \"my never\", \"not\", \"now of\", \"on\", \"only\", \"other\", \"our\", \"out\", \"or\", \"over\", \"said\", \"same\", \"see\", \"should\", \"since\", \"she\", \"some\", \"still\", \"such\", \"take\", \"than\", \"that\", \"the\", \"their\", \"them\", \"then\", \"there\", \"these\", \"therefore\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"thus\", \"under\", \"up\", \"was\", \"way\", \"we\", \"well\", \"were\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"will\", \"with\", \"would\", \"your\", \"null\"]\n",
    "    words = input_text.split() # Split words by space\n",
    "    noise_free_words = [word for word in words if word.lower() not in noise_list] #Get a list of non-noise words\n",
    "    noise_free_text = \" \".join(noise_free_words) #Get a string of non-noise words\n",
    "    return noise_free_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_regex(input_text):\n",
    "    #split tweet by space\n",
    "    words = input_text.split() \n",
    "    regex_free_text=\"\"\n",
    "    #check if word is alpha(contain letters only) , then add it to regex_free_text\n",
    "    for word in words:\n",
    "      if word.isalpha():\n",
    "        regex_free_text += word\n",
    "        regex_free_text +=\" \"\n",
    "    return regex_free_text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(filename):\n",
    "    my_file = open(filename, mode='r')\n",
    "    #return value of csv file is an iterator \n",
    "    read = csv.reader(my_file,delimiter='\\t')\n",
    "    #splitting = read.split('\\t')\n",
    "    flag=0;\n",
    "    #flag ---> used to skip the header of the file\n",
    "    #column one for tweets , column 2 for our ouput(NOT or OFF)\n",
    "    for row in read:\n",
    "      if flag ==0:\n",
    "        flag=1\n",
    "        continue;\n",
    "      tweets.append(row[1])\n",
    "      labels.append(row[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features():   \n",
    "   vectorizer = TfidfVectorizer()\n",
    "   X = vectorizer.fit_transform(tweets)\n",
    "   print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 710)\t0.4528123798633217\n",
      "  (0, 4889)\t0.4711243893658061\n",
      "  (0, 8661)\t0.6129130060569749\n",
      "  (0, 439)\t0.4442301266161649\n",
      "  (1, 5539)\t0.5578229144311855\n",
      "  (1, 6175)\t0.7514648189973556\n",
      "  (1, 13824)\t0.3523268680426881\n",
      "  (2, 13824)\t0.10361648931575587\n",
      "  (2, 423)\t0.3137045751433941\n",
      "  (2, 6844)\t0.3317472359017649\n",
      "  (2, 2214)\t0.28585579854793897\n",
      "  (2, 4291)\t0.2971037711837856\n",
      "  (2, 11499)\t0.27733781319931966\n",
      "  (2, 6793)\t0.32158613630530003\n",
      "  (2, 3212)\t0.2799553502586449\n",
      "  (2, 11498)\t0.3460685014268144\n",
      "  (2, 7771)\t0.2086071249718249\n",
      "  (2, 4153)\t0.3072648707802506\n",
      "  (2, 2566)\t0.3072648707802506\n",
      "  (3, 12112)\t0.5333100472276878\n",
      "  (3, 9647)\t0.6611252697557053\n",
      "  (3, 9017)\t0.24011266794431402\n",
      "  (3, 11691)\t0.46993582318132643\n",
      "  (4, 8960)\t0.3548753570066678\n",
      "  (4, 14125)\t0.38438864441132187\n",
      "  :\t:\n",
      "  (13233, 13222)\t0.440480016697724\n",
      "  (13234, 5087)\t0.5421040718330896\n",
      "  (13234, 2577)\t0.544553932604619\n",
      "  (13234, 10940)\t0.6399876481517659\n",
      "  (13235, 9017)\t0.20691700320472586\n",
      "  (13235, 9529)\t0.16505886879338025\n",
      "  (13235, 8294)\t0.30132206368366776\n",
      "  (13235, 12589)\t0.2881271018027258\n",
      "  (13235, 3189)\t0.29553660476910165\n",
      "  (13235, 12117)\t0.31184660756423704\n",
      "  (13235, 8573)\t0.3352821936413025\n",
      "  (13235, 13947)\t0.3812898714292897\n",
      "  (13235, 13948)\t0.3812898714292897\n",
      "  (13235, 13009)\t0.4103174842384595\n",
      "  (13236, 9017)\t0.1256664073050787\n",
      "  (13236, 7891)\t0.27581122513717016\n",
      "  (13236, 12699)\t0.4278771853868789\n",
      "  (13236, 11593)\t0.4777693429872924\n",
      "  (13236, 1202)\t0.49839426726752223\n",
      "  (13236, 2989)\t0.49839426726752223\n",
      "  (13237, 5501)\t0.5584119818712194\n",
      "  (13237, 14327)\t0.4327557583545005\n",
      "  (13237, 10814)\t0.7077418400191401\n",
      "  (13238, 10347)\t1.0\n",
      "  (13239, 13824)\t1.0\n"
     ]
    }
   ],
   "source": [
    "readFile(\"offenseval-training-v1.tsv\")\n",
    "#loop for each tweet remove regex & noise\n",
    "for tweet in range(0,len(tweets)):\n",
    "    tweets[tweet]=remove_regex(tweets[tweet])\n",
    "    tweets[tweet]=remove_noise(tweets[tweet])\n",
    "#to extract Features\n",
    "extract_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
